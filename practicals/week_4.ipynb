{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In this practical session you will implement and train several Convolutional Neural Networks (CNNs) using the Keras framework with a Tensorflow backend. If you are not already familiar with Keras, you can go over the [following tutorial](https://github.com/tueimage/essential-skills/blob/master/keras.md). More detailed information on the different functionalities can be found in the [Keras library documentation](https://keras.io/). \n",
    "\n",
    "Note that for this set of exercise CPU-only Tensorflow is sufficient (i.e. GPU-support is not required but it will make your experiments run faster). If you use the Anaconda Python distribution, it is recommeded to install Tensorflow via the conda package manager: `conda install tensorflow-cpu`.\n",
    "\n",
    "You are also required to use the `gryds` package for data augmentation that you can install directly from git: `pip install git+https://github.com/tueimage/gryds/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "We will first train a simple CNN to classify handwritten digits using the MNIST dataset. This dataset is often referred to as the \"Hello world!\" example of deep learning because it can be used to quickly illustrate a small neural network in action (and obtain a decent classification accuracy in the process). More information on it can be found [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "First, let's load the dataset and visualize some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load the MNIST the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# scale the image intensities to the 0-1 range\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# convert the data to channel-last\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# convert the labels to one-hot encoded\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "def plot_images(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_images(x_train[np.random.randint(0, x_train.shape[0], size=100)].reshape(100, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST classification task is quite simple: given an image, predict the digit that it contains. Thus, this is a 10-class classification problem.\n",
    "\n",
    "Let's define a simple network for the handwritten digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the network (note that this could take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate its performance on the independent test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "The U-Net convolutional neural network architecture was first developed for biomedical image segmentation and is to this day one of the most widely used methods for image segmentation. The details of the architecture can be found in the [original paper](https://arxiv.org/abs/1505.04597). In this practical we will build and train a U-Net network that is able to segment blood vessels in retinal images. \n",
    "\n",
    "### Loading and visualizing the data\n",
    "The data for this task is taken from the [DRIVE](https://www.isi.uu.nl/Research/Databases/DRIVE/index.php) database. It consists of photographs of the retina, where the goal is to segment the blood vessels within. The dataset has a total of 40 photographs, divided in 20 images for training and 20 for testing. \n",
    "\n",
    "Let's load the training set and visualize an image with the corresponding blood vessel segmentation. For training we will divide the data in a training and a validation set to monitor the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-72c4787477a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Image of the retina\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3224bc0320>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"code/\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from unet_utils import load_data\n",
    "\n",
    "# location of the DRIVE dataset\n",
    "data_folder = r'C:\\Users\\s140014\\Documents\\BMT\\Master\\Q1\\8dm40-machine-learning\\practicals\\data\\DRIVE/'\n",
    "train_paths = glob(data_folder + 'training/images/*.tif')\n",
    "images, masks, segmentations = load_data(train_paths)\n",
    "\n",
    "# print the shape of image dataset\n",
    "print(images.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(segmentations[0][:, :, 0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# divide in training and validation\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "# print the shape of the training and valudation datasets\n",
    "print(train_images.shape)\n",
    "print(train_masks.shape)\n",
    "print(train_segmentations.shape)\n",
    "print(val_images.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_segmentations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a U-Net model\n",
    "\n",
    "You are already provided with implementation of the U-Net architecture in `unet_model.py`. This is a modular implementation and can be used to generate U-Net architectures with a variety of hyperparameters such as depth and number of feature maps. Before using the model, examine the code and documentation and make sure that you understand all the details.\n",
    "\n",
    "We will train a U-Net model using smaller patches extracted from the training images. Training the images on smaller patches requires less computation power and results in a more varied training dataset (it has the effect of data augmentation by image translation). Because a U-Net is a fully convolutional network it can be evaluated on inputs of different size (the output size will change according to the input size). Thus, although the model will be trained on smaller patches it can still be used to segment larger images with one pass through the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_utils import extract_patches, preprocessing\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# visualize a couple of patches as a visual check\n",
    "patches, patches_segmentations = extract_patches(train_images, train_segmentations, patch_size, patches_per_im=1, seed=7)\n",
    "\n",
    "print(patches.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 12))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(patches[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(patches_segmentations[i][:, :, 0])\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].imshow(patches[i+5])\n",
    "    axes[i, 3].axis('off')\n",
    "    axes[i, 3].imshow(patches_segmentations[i+5][:, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the validation data to fit the U-Net model\n",
    "# images of shape (584, 565) shape result in concatenation error due to the odd number of columns\n",
    "\n",
    "print(\"Old shape:\", val_images.shape)\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "    \n",
    "print(\"New shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from unet_model import unet\n",
    "from unet_utils import datagenerator\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# use a single training image, to better demonstrate the effects of data augmentation\n",
    "X_train, y_train = np.expand_dims(train_images[0], axis=0), np.expand_dims(train_segmentations[0], axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# hyperparameters\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(train_images)) / batch_size))\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# initialize model\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "# print a summary of the model\n",
    "# model.summary(line_length=120)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# train the model with the data generator, and save the training history\n",
    "history = model.fit_generator(datagenerator(X_train, y_train, patch_size, patches_per_im, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=2,\n",
    "                              callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on one test image and show the results\n",
    "from unet_utils import preprocessing\n",
    "\n",
    "# test data paths\n",
    "impaths_test = glob(data_folder + 'test/images/*.tif')\n",
    "\n",
    "# load data\n",
    "test_images, test_masks, test_segmentations = load_data(impaths_test, test=True)\n",
    "\n",
    "# pad the data to fit the U-Net model\n",
    "test_images, test_masks, test_segmentations = preprocessing(test_images, test_masks, test_segmentations, \n",
    "                                                            desired_shape=(584, 584))\n",
    "\n",
    "# use a single image to evaluate\n",
    "X_test, y_test = np.expand_dims(test_images[0], axis=0), np.expand_dims(test_masks[0], axis=0)\n",
    "\n",
    "# predict test samples\n",
    "test_prediction = model.predict(X_test, batch_size=4)\n",
    "\n",
    "# visualize the test result\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_images[0])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_segmentations[0][:, :, 0])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_prediction[0, :, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Number of parameters\n",
    "\n",
    "The first convolutional layer in the MNIST example has 320 parameters. The first fully connected layer has 1179,776 parameters. What do these parameters correspond to? \n",
    "\n",
    "<font color='#770a0a'>What is the general expression for the number of parameters of 1) a convolutional layer and 2) a fully-connected layer?</font>\n",
    "\n",
    "## Fully-convolutional MNIST model\n",
    "\n",
    "Modify the model in the MNIST example in such a way that it only contains convolutional layers while keeping the same number of parameters. If you do the modification correctly, the two models will have the same behaviour (i.e. they will represent the same model, only with different implementation). Show this experimentally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net architecture\n",
    "\n",
    "<font color='#770a0a'> What is the role of the skip connections in the U-Net neural network architecture? Will it be possible to train the exact same architecture with the skip connections omitted? If yes, what would be the expected result? If no, what would be the cause of the error?</font>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "<font color='#770a0a'>Why does data augmentation result in less overfitting? Can data augmentation be applied to the test samples? If yes, towards what goal? If no, what is preventing that?</font>\n",
    "\n",
    "\n",
    "Implement random brightness augmentation of the image data by adding a random offset to the image intensity before passing them trough the network at training time. Train a model with random brightness augmentation and compare it to the baseline above. \n",
    "\n",
    "Implement data augmentation procedure that in addition to brightness augmentation also performs b-spline geometric augmentation using the [`gryds`](https://github.com/tueimage/gryds) package (you can look at the documentation of the package for an example on how to do that). Compare the new model with the baseline and the model that only performs brightness augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
